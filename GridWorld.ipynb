{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvAjhwNRvpjLKpM5n2P4az",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srikarraju/GridWorld/blob/main/GridWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVJapZc6qTRp"
      },
      "source": [
        "import numpy as np\n",
        "class Gridworld:\n",
        "  def __init__(self,grid_dim=4):\n",
        "    self.grid_dim = grid_dim\n",
        "    self.number_states = self.grid_dim*self.grid_dim - 1\n",
        "    self.curr_state = 0\n",
        "\n",
        "  def reset(self):\n",
        "    self.curr_state = 0\n",
        "    return self.curr_state\n",
        "\n",
        "  def step(self,state,action):\n",
        "    curr_state = (int(state/self.grid_dim),state%self.grid_dim)\n",
        "    if action == 0:#Left\n",
        "      if curr_state[1]-1<0:\n",
        "        return state,-1,0\n",
        "      else:\n",
        "        next_state = (curr_state[0],curr_state[1]-1)\n",
        "        if next_state[0]==self.grid_dim-1 and next_state[1]==self.grid_dim-1:\n",
        "          return next_state[0]*4 + next_state[1],0,1\n",
        "        else:\n",
        "          return next_state[0]*4 + next_state[1],-1,0\n",
        "    elif action ==1:#Right\n",
        "      if curr_state[1]+1>=self.grid_dim:\n",
        "        return state,-1,0\n",
        "      else:\n",
        "        next_state = (curr_state[0],curr_state[1]+1)\n",
        "        if next_state[0]==self.grid_dim-1 and next_state[1]==self.grid_dim-1:\n",
        "          return next_state[0]*4 + next_state[1],0,1\n",
        "        else:\n",
        "          return next_state[0]*4 + next_state[1],-1,0\n",
        "    elif action ==2:#Up\n",
        "      if curr_state[0]-1<0:\n",
        "        return state,-1,0\n",
        "      else:\n",
        "        next_state = (curr_state[0]-1,curr_state[1])\n",
        "        if next_state[0]==self.grid_dim-1 and next_state[1]==self.grid_dim-1:\n",
        "          return next_state[0]*4 + next_state[1],0,1\n",
        "        else:\n",
        "          return next_state[0]*4 + next_state[1],-1,0\n",
        "    elif action ==3:#Down\n",
        "      if curr_state[0]+1>=self.grid_dim:\n",
        "        return state,-1,0\n",
        "      else:\n",
        "        next_state = (curr_state[0]+1,curr_state[1])\n",
        "        if next_state[0]==self.grid_dim-1 and next_state[1]==self.grid_dim-1:\n",
        "          return next_state[0]*4 + next_state[1],0,1\n",
        "        else:\n",
        "          return next_state[0]*4 + next_state[1],-1,0\n",
        "    else:\n",
        "      print(\"Invalid Action\")\n",
        "      return None,0,1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVsj6XjfCYc6"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "\n",
        "rng = default_rng()\n",
        "\n",
        "def generate_state_features(num_states,d,l):\n",
        "  state_features = []\n",
        "  product_dict = {}\n",
        "  while len(state_features) < num_states:\n",
        "    one_indices = rng.choice(d, size = l, replace=False)\n",
        "    product = 1\n",
        "    for i in range(l):\n",
        "      product *= one_indices[i]\n",
        "    feature_vec = np.zeros(d)\n",
        "    if product not in product_dict:\n",
        "      for index in one_indices:\n",
        "        feature_vec[index] = 1\n",
        "      state_features.append(feature_vec)\n",
        "      product_dict[product] = 1\n",
        "  #print(len(product_dict))\n",
        "  return state_features\n",
        "\n",
        "#print(np.asarray(generate_state_features(16,8,4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o84fCuK8AYuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae2eaef-bf5c-4343-90bf-8a720446bba7"
      },
      "source": [
        "def generate_state_action_features(state_vec, d, num_actions):\n",
        "  state_action_features = np.zeros(shape=(num_actions,d*num_actions))\n",
        "  for i in range(num_actions):\n",
        "    for j in range(d):\n",
        "      state_action_features[i][d*i+j] = state_vec[j]\n",
        "  return state_action_features\n",
        "\n",
        "print(generate_state_action_features([1,0],2,4))\n",
        "\n",
        "# def generate_state_action_features(state_vec, d, num_actions):\n",
        "#   state_action_features = []\n",
        "#   for i in range(num_actions):\n",
        "#     state_action_features.append(state_vec)\n",
        "#     state_action_features[i] = np.append(state_action_features[i],i)\n",
        "#   return np.asarray(state_action_features)\n",
        "\n",
        "#print(generate_state_action_features(np.asarray([1,0]),2,4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5DrzYCjcskG"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "im\n",
        "import torch.nn as nnport gym\n",
        "from collections import deque\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1A5HeAldTci",
        "outputId": "c386f934-ed04-416b-d644-48199e90d91e"
      },
      "source": [
        "grid_dim = 4\n",
        "num_actions = 4\n",
        "d = 2\n",
        "\n",
        "env = Gridworld(grid_dim)\n",
        "#print(env)\n",
        "returns = deque(maxlen=100)\n",
        "weights_v = np.zeros(2)\n",
        "weights_p = np.zeros(d*num_actions)\n",
        "weights_w = np.zeros(d*num_actions)\n",
        "\n",
        "print(weights_v)\n",
        "print(weights_p)\n",
        "print(weights_w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0W1Z-GG4d2i",
        "outputId": "3116b011-bf13-4ab7-bede-84b0d9a759e3"
      },
      "source": [
        "alpha_0, beta_0, gamma, epsilon = 0.1, 0.01, 0.95, 0.1\n",
        "alpha_c, beta_c = 1000, 100000\n",
        "t = 0\n",
        "n_episode = 0\n",
        "actions_list = []\n",
        "avg_reward = 0\n",
        "\n",
        "state_features = np.array([[0,0],[0,1],[0,2],[0,3],\n",
        "                           [1,0],[1,1],[1,2],[1,3],\n",
        "                           [2,0],[2,1],[2,2],[2,3],\n",
        "                           [3,0],[3,1],[3,2],[3,3]\n",
        "                           ])\n",
        "while n_episode <=5000:\n",
        "  rewards,states,actions = [],[],[]\n",
        "  state = 0\n",
        "  t = 0\n",
        "  episode_len = 0\n",
        "  while episode_len<=100:\n",
        "    episode_len += 1\n",
        "    state_action_features = generate_state_action_features(state_features[state],d,num_actions)\n",
        "    t += 1\n",
        "\n",
        "    probs = np.dot(weights_p,np.transpose(state_action_features))\n",
        "\n",
        "    probs -= probs.max()\n",
        "    probs = np.exp(np.clip(probs/epsilon, -500, 500))\n",
        "    probs /= probs.sum()\n",
        "    probs2 = probs.cumsum()\n",
        "    action = np.where(probs2 >= np.random.random())[0][0]\n",
        "\n",
        "    probs3 = -1*probs\n",
        "    probs3[action] += 1\n",
        "    features = state_features[state]\n",
        "    features = np.repeat(features.reshape((features.size,1)), num_actions, axis=1)\n",
        "    compat_features = np.dot(features, np.diag(probs3))\n",
        "\n",
        "\n",
        "\n",
        "    new_state, reward, done = env.step(state,action)\n",
        "\n",
        "\n",
        "    value_curr = np.dot(weights_v,np.asarray(state_features[state]))\n",
        "    value_next = np.dot(weights_v,np.asarray(state_features[new_state]))\n",
        "\n",
        "    avg_reward = (1 - gamma)*avg_reward + gamma * reward\n",
        "\n",
        "    #td_error = reward + value_curr - value_next - avg_reward\n",
        "\n",
        "    td_error = reward + value_curr - value_next\n",
        "\n",
        "    beta = (beta_0 * beta_c) / (beta_c + t)\n",
        "    alpha = (alpha_0 * alpha_c) / (alpha_c + t**(2/3))\n",
        "\n",
        "    weights_v += alpha * (abs(td_error)/td_error) * np.asarray(state_features[state])\n",
        "\n",
        "    grad_prob = (1 - probs[action]) * np.asarray(state_action_features[action])\n",
        "\n",
        "    weights_w = np.dot(np.eye(compat_features.size) - beta * np.outer(compat_features, compat_features), weights_w)\n",
        "    weights_p += beta * weights_w\n",
        "\n",
        "\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    actions_list.append(action)\n",
        "\n",
        "    state = new_state\n",
        "    if done == 1:\n",
        "      break\n",
        "\n",
        "  returns.append(np.sum(rewards))\n",
        "  #print(np.sum(rewards))\n",
        "  if n_episode%100==0:\n",
        "    print(\"Episode: {:6d}\\tAvg. Return: {:6.2f}\".format(n_episode, np.mean(returns)))\n",
        "  n_episode += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode:      0\tAvg. Return: -37.00\n",
            "Episode:    100\tAvg. Return: -52.99\n",
            "Episode:    200\tAvg. Return: -52.93\n",
            "Episode:    300\tAvg. Return: -49.56\n",
            "Episode:    400\tAvg. Return: -53.93\n",
            "Episode:    500\tAvg. Return: -48.31\n",
            "Episode:    600\tAvg. Return: -55.17\n",
            "Episode:    700\tAvg. Return: -51.36\n",
            "Episode:    800\tAvg. Return: -50.70\n",
            "Episode:    900\tAvg. Return: -47.63\n",
            "Episode:   1000\tAvg. Return: -46.54\n",
            "Episode:   1100\tAvg. Return: -55.28\n",
            "Episode:   1200\tAvg. Return: -52.02\n",
            "Episode:   1300\tAvg. Return: -45.62\n",
            "Episode:   1400\tAvg. Return: -53.11\n",
            "Episode:   1500\tAvg. Return: -52.50\n",
            "Episode:   1600\tAvg. Return: -49.16\n",
            "Episode:   1700\tAvg. Return: -55.78\n",
            "Episode:   1800\tAvg. Return: -50.07\n",
            "Episode:   1900\tAvg. Return: -57.08\n",
            "Episode:   2000\tAvg. Return: -50.88\n",
            "Episode:   2100\tAvg. Return: -50.67\n",
            "Episode:   2200\tAvg. Return: -52.33\n",
            "Episode:   2300\tAvg. Return: -52.47\n",
            "Episode:   2400\tAvg. Return: -49.92\n",
            "Episode:   2500\tAvg. Return: -47.24\n",
            "Episode:   2600\tAvg. Return: -53.02\n",
            "Episode:   2700\tAvg. Return: -49.14\n",
            "Episode:   2800\tAvg. Return: -49.52\n",
            "Episode:   2900\tAvg. Return: -51.65\n",
            "Episode:   3000\tAvg. Return: -47.61\n",
            "Episode:   3100\tAvg. Return: -55.07\n",
            "Episode:   3200\tAvg. Return: -51.60\n",
            "Episode:   3300\tAvg. Return: -46.90\n",
            "Episode:   3400\tAvg. Return: -47.29\n",
            "Episode:   3500\tAvg. Return: -47.95\n",
            "Episode:   3600\tAvg. Return: -51.47\n",
            "Episode:   3700\tAvg. Return: -49.06\n",
            "Episode:   3800\tAvg. Return: -52.33\n",
            "Episode:   3900\tAvg. Return: -52.85\n",
            "Episode:   4000\tAvg. Return: -49.41\n",
            "Episode:   4100\tAvg. Return: -50.17\n",
            "Episode:   4200\tAvg. Return: -48.61\n",
            "Episode:   4300\tAvg. Return: -51.66\n",
            "Episode:   4400\tAvg. Return: -56.98\n",
            "Episode:   4500\tAvg. Return: -50.85\n",
            "Episode:   4600\tAvg. Return: -51.85\n",
            "Episode:   4700\tAvg. Return: -50.88\n",
            "Episode:   4800\tAvg. Return: -48.68\n",
            "Episode:   4900\tAvg. Return: -50.21\n",
            "Episode:   5000\tAvg. Return: -50.53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "Li2iufLOfgkX",
        "outputId": "c18482e6-b885-4dcc-ae33-f30e70996e7f"
      },
      "source": [
        "alpha_0, beta_0, gamma, epsilon = 0.1, 0.01, 0.95, 0.1\n",
        "alpha_c, beta_c = 1000, 100000\n",
        "t = 0\n",
        "n_episode = 0\n",
        "actions_list = []\n",
        "avg_reward = 0\n",
        "\n",
        "state_features = np.array([[0,0],[0,1],[0,2],[0,3],\n",
        "                           [1,0],[1,1],[1,2],[1,3],\n",
        "                           [2,0],[2,1],[2,2],[2,3],\n",
        "                           [3,0],[3,1],[3,2],[3,3]\n",
        "                           ])\n",
        "while n_episode <=500000:\n",
        "  rewards,states,actions = [],[],[]\n",
        "  state = env.reset()\n",
        "\n",
        "  state = 0\n",
        "  value_curr = np.dot(state_features[state],weights_v)\n",
        "  episode_len = 0\n",
        "  #t = 0\n",
        "  #print(n_episode)\n",
        "  #print(value_curr)\n",
        "  while True:\n",
        "    t += 1\n",
        "    episode_len += 1\n",
        "    state_action_features = generate_state_action_features(state_features[state],d,num_actions)\n",
        "    probs = np.dot(np.asarray(state_action_features),weights_p)\n",
        "    probs -= probs.max()\n",
        "    probs = np.exp(np.clip(probs/epsilon, -50, 50))\n",
        "    probs /= probs.sum()\n",
        "\n",
        "    probs2 = probs.cumsum()\n",
        "    action = np.where(probs2 >= np.random.random())[0][0]\n",
        "    #print(action)\n",
        "\n",
        "    new_state, reward, done = env.step(state,action)\n",
        "\n",
        "\n",
        "    value_curr = np.dot(weights_v,np.asarray(state_features[state]))\n",
        "    value_next = np.dot(weights_v,np.asarray(state_features[new_state]))\n",
        "\n",
        "    beta = (beta_0 * beta_c) / (beta_c + t)\n",
        "    alpha = (alpha_0 * alpha_c) / (alpha_c + t**(2/3))\n",
        "\n",
        "    avg_reward = (1 - gamma*alpha)*avg_reward + gamma*alpha * reward\n",
        "\n",
        "    td_error = reward + value_next - value_curr - avg_reward\n",
        "\n",
        "    weights_v += 0.01*alpha * td_error * np.asarray(state_features[state])\n",
        "\n",
        "    grad_prob = probs[action]*(state_action_features[action] - np.dot(np.transpose(state_action_features),probs)) #change this as probs is cum sum\n",
        "\n",
        "    weights_w = np.dot((np.eye(d+1) - alpha * np.outer(grad_prob,grad_prob) ),weights_w) + alpha * td_error * grad_prob\n",
        "\n",
        "    weights_p += beta * weights_w\n",
        "\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    actions_list.append(action)\n",
        "\n",
        "    state = new_state\n",
        "    if done==1 or episode_len>=200:\n",
        "      break\n",
        "  n_episode += 1\n",
        "  returns.append(np.sum(rewards))\n",
        "  #print(np.sum(rewards))\n",
        "  if n_episode%100==0:\n",
        "    print(\"Episode: {:6d}\\tAvg. Return: {:6.2f}\".format(n_episode, np.mean(returns)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1b5e4921ea1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mepisode_len\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mepisode_len\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mstate_action_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_state_action_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'state_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QOaFuf4B24n",
        "outputId": "2e2d0dfb-4097-4159-c059-8fd6a8e7139b"
      },
      "source": [
        "print(np.dot(state_features,weights_v))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.         -0.17910808 -0.35821617 -0.53732425  1.15368516  0.97457707\n",
            "  0.79546899  0.61636091  2.30737031  2.12826223  1.94915414  1.77004606\n",
            "  3.46105547  3.28194738  3.1028393   2.92373122]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyt_E-EntKXV",
        "outputId": "b5b0ea7b-4dea-44b3-86f8-24712ae42c49"
      },
      "source": [
        "print(states)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 4, 8, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_3TJUTevZrG",
        "outputId": "1490e379-7c0a-400f-822a-ab85e45f8761"
      },
      "source": [
        "state = 12\n",
        "probs = np.dot(generate_state_action_features(state_features[state],d,num_actions),weights_p)\n",
        "probs -= probs.max()\n",
        "\n",
        "probs = np.exp(np.clip(probs/epsilon, -50, 50))\n",
        "probs /= probs.sum()\n",
        "print(probs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.92874985e-22 1.92874985e-22 1.92874985e-22 1.00000000e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMHOvHQD3qtu"
      },
      "source": [
        "env = Gridworld(4)\n",
        "state = env.reset()\n",
        "total_reward = 0.0\n",
        "for _ in range(10000):\n",
        "  #print(state)\n",
        "  action = np.random.randint(0,4)\n",
        "  #print(action)\n",
        "  next_state, reward, done  = env.step(state,action)#take a random action\n",
        "  total_reward += reward\n",
        "  state = next_state\n",
        "  if done== 1:\n",
        "    #print(action)\n",
        "    print(total_reward)\n",
        "    state = env.reset()\n",
        "    total_reward = 0"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}