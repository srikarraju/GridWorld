{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgVWQh6/M/KbD4u60AJ+rk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srikarraju/GridWorld/blob/main/Natural_Actor_critic_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5DrzYCjcskG"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import gym\n",
        "from collections import deque\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJtZQqSLj44d"
      },
      "source": [
        "class value_net(nn.Module):\n",
        "  def __init__(self,state_dim,hidden_dim):\n",
        "    super(value_net,self).__init__()\n",
        "    self.h = nn.Linear(state_dim,hidden_dim)\n",
        "    self.out = nn.Linear(hidden_dim,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.h(x)\n",
        "    x = nn.ReLU()(x)\n",
        "    x = self.out(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzMCyJ7oj_ka",
        "outputId": "5c081914-3330-45ce-b32a-19b0462535f5"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "value_fn = value_net(4,8)\n",
        "optimizer2 = torch.optim.Adam(value_fn.parameters())\n",
        "\n",
        "weights_w = np.zeros(5,dtype=float)\n",
        "weights_p = np.zeros(5,dtype=float)\n",
        "print(weights_w)\n",
        "print(weights_p)\n",
        "\n",
        "returns = deque(maxlen=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GvNA6ZVhkLLV",
        "outputId": "b4a981b4-4949-4c8d-a606-bd1ec65b8002"
      },
      "source": [
        "alpha_0, beta_0, gamma, epsilon = 0.1, 0.01, 0.95, 0.1\n",
        "alpha_c, beta_c = 1000, 1000\n",
        "t = 0\n",
        "n_episode = 1\n",
        "actions_list = []\n",
        "avg_reward = 0\n",
        "\n",
        "fischer_inv = 1.5*np.eye(5)\n",
        "\n",
        "while n_episode <=5000:\n",
        "  rewards,states,actions = [],[],[]\n",
        "  state = env.reset()\n",
        "\n",
        "  while True:\n",
        "    t += 1\n",
        "    state_action = np.zeros((2,5),dtype=float)\n",
        "    for i in range(4):\n",
        "      state_action[0][i] = state[i]\n",
        "      state_action[1][i] = state[i]\n",
        "    state_action[0][4] = 0\n",
        "    state_action[1][4] = 1\n",
        "\n",
        "    probs = np.dot(np.asarray(state_action),weights_p)\n",
        "    probs -= probs.max()\n",
        "    probs = np.exp(np.clip(probs/epsilon, -500, 500))\n",
        "    probs /= probs.sum()\n",
        "\n",
        "    probs2 = probs.cumsum()\n",
        "    action = np.where(probs2 >= np.random.random())[0][0]\n",
        "    #print(action)\n",
        "\n",
        "    new_state, reward, done, info = env.step(action.item())\n",
        "\n",
        "\n",
        "    value_curr = value_fn(torch.tensor(state).unsqueeze(0).float())\n",
        "    value_next = value_fn(torch.tensor(state).unsqueeze(0).float())\n",
        "\n",
        "\n",
        "    beta = (beta_0 * beta_c) / (beta_c + t)\n",
        "    alpha = (alpha_0 * alpha_c) / (alpha_c + t**(2/3))\n",
        "\n",
        "    avg_reward = (1 - gamma)*avg_reward + gamma * reward\n",
        "    td_error = reward + value_next - value_curr -avg_reward\n",
        "    td_error = td_error.detach()\n",
        "    td_error = td_error.item()\n",
        "\n",
        "    value_net_loss = -0.1*alpha* td_error*value_curr\n",
        "    optimizer2.zero_grad()\n",
        "    value_net_loss.backward()\n",
        "    optimizer2.step()\n",
        "\n",
        "    grad_prob = probs[action]*(state_action[action] - np.dot(np.transpose(state_action),probs))\n",
        "\n",
        "    Gt_psi_dot = np.dot(fischer_inv, grad_prob)\n",
        "    fischer_inv -= (0.001*alpha * np.outer(Gt_psi_dot,np.transpose(Gt_psi_dot)) ) / (1 - 0.001*alpha + 0.001*alpha * np.dot(grad_prob,Gt_psi_dot))\n",
        "    fischer_inv /= (1 - 0.001*alpha)\n",
        "\n",
        "    weights_w = (1-alpha)*weights_w + alpha*td_error*np.dot(fischer_inv, grad_prob)\n",
        "\n",
        "    weights_p += beta * weights_w\n",
        "\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    actions_list.append(action)\n",
        "\n",
        "    state = new_state\n",
        "    if done==True:\n",
        "      break\n",
        "\n",
        "  returns.append(np.sum(rewards))\n",
        "  #print(np.sum(rewards))\n",
        "  if n_episode%100==0:\n",
        "    print(\"Episode: {:6d}\\tAvg. Return: {:6.2f}\".format(n_episode, np.mean(returns)))\n",
        "  #reinforce_baseline_returns.append(np.mean(returns))\n",
        "  n_episode += 1\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode:    100\tAvg. Return:  21.76\n",
            "Episode:    200\tAvg. Return:  21.25\n",
            "Episode:    300\tAvg. Return:  21.08\n",
            "Episode:    400\tAvg. Return:  24.67\n",
            "Episode:    500\tAvg. Return:  21.81\n",
            "Episode:    600\tAvg. Return:  22.92\n",
            "Episode:    700\tAvg. Return:  22.27\n",
            "Episode:    800\tAvg. Return:  22.63\n",
            "Episode:    900\tAvg. Return:  22.81\n",
            "Episode:   1000\tAvg. Return:  22.02\n",
            "Episode:   1100\tAvg. Return:  21.51\n",
            "Episode:   1200\tAvg. Return:  22.79\n",
            "Episode:   1300\tAvg. Return:  23.21\n",
            "Episode:   1400\tAvg. Return:  22.35\n",
            "Episode:   1500\tAvg. Return:  21.05\n",
            "Episode:   1600\tAvg. Return:  23.23\n",
            "Episode:   1700\tAvg. Return:  20.99\n",
            "Episode:   1800\tAvg. Return:  21.09\n",
            "Episode:   1900\tAvg. Return:  22.66\n",
            "Episode:   2000\tAvg. Return:  22.48\n",
            "Episode:   2100\tAvg. Return:  22.15\n",
            "Episode:   2200\tAvg. Return:  22.54\n",
            "Episode:   2300\tAvg. Return:  22.01\n",
            "Episode:   2400\tAvg. Return:  21.62\n",
            "Episode:   2500\tAvg. Return:  23.16\n",
            "Episode:   2600\tAvg. Return:  23.79\n",
            "Episode:   2700\tAvg. Return:  22.50\n",
            "Episode:   2800\tAvg. Return:  22.32\n",
            "Episode:   2900\tAvg. Return:  20.76\n",
            "Episode:   3000\tAvg. Return:  22.08\n",
            "Episode:   3100\tAvg. Return:  21.87\n",
            "Episode:   3200\tAvg. Return:  22.38\n",
            "Episode:   3300\tAvg. Return:  21.78\n",
            "Episode:   3400\tAvg. Return:  23.11\n",
            "Episode:   3500\tAvg. Return:  21.71\n",
            "Episode:   3600\tAvg. Return:  22.49\n",
            "Episode:   3700\tAvg. Return:  21.62\n",
            "Episode:   3800\tAvg. Return:  21.71\n",
            "Episode:   3900\tAvg. Return:  22.45\n",
            "Episode:   4000\tAvg. Return:  21.82\n",
            "Episode:   4100\tAvg. Return:  22.77\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c415b12563d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0moptimizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mvalue_net_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0moptimizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mgrad_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    group['eps'])\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1A5HeAldTci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b80d87-12cb-4891-9c98-e3bb8d4e598e"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "weights_v = np.zeros(4,dtype=float)\n",
        "weights_w = np.zeros(5,dtype=float)\n",
        "weights_p = np.zeros(5,dtype=float)\n",
        "print(weights_v)\n",
        "print(weights_p)\n",
        "\n",
        "returns = deque(maxlen=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li2iufLOfgkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec3c9406-bee9-4efc-f1b0-030f906ea721"
      },
      "source": [
        "alpha_0, beta_0, gamma, epsilon = 0.1, 0.01, 0.95, 0.1\n",
        "alpha_c, beta_c = 1000, 1000\n",
        "t = 0\n",
        "n_episode = 1\n",
        "actions_list = []\n",
        "avg_reward = 0\n",
        "\n",
        "fischer_inv = 1.5*np.eye(5)\n",
        "\n",
        "while n_episode <=5000:\n",
        "  rewards,states,actions = [],[],[]\n",
        "  state = env.reset()\n",
        "\n",
        "  state = np.asarray(state)\n",
        "  value_curr = np.dot(weights_v,state)\n",
        "  #print(value_curr)\n",
        "\n",
        "  while True:\n",
        "    t += 1\n",
        "    state_action = np.zeros((2,5),dtype=float)\n",
        "    for i in range(4):\n",
        "      state_action[0][i] = state[i]\n",
        "      state_action[1][i] = state[i]\n",
        "    state_action[0][4] = 0\n",
        "    state_action[1][4] = 1\n",
        "\n",
        "    probs = np.dot(np.asarray(state_action),weights_p)\n",
        "    probs -= probs.max()\n",
        "    probs = np.exp(np.clip(probs/epsilon, -500, 500))\n",
        "    probs /= probs.sum()\n",
        "\n",
        "    probs2 = probs.cumsum()\n",
        "    action = np.where(probs2 >= np.random.random())[0][0]\n",
        "    #print(action)\n",
        "\n",
        "    new_state, reward, done, info = env.step(action.item())\n",
        "\n",
        "\n",
        "    value_curr = np.dot(weights_v,np.asarray(state))\n",
        "    value_next = np.dot(weights_v,np.asarray(new_state))\n",
        "\n",
        "    avg_reward = (1 - gamma)*avg_reward + gamma * reward\n",
        "\n",
        "    td_error = reward + value_curr - value_next - avg_reward\n",
        "\n",
        "    beta = (beta_0 * beta_c) / (beta_c + t)\n",
        "    alpha = (alpha_0 * alpha_c) / (alpha_c + t**(2/3))\n",
        "\n",
        "    weights_v += alpha * td_error * np.asarray(state)\n",
        "\n",
        "    grad_prob = probs[action]*(state_action[action] - np.dot(np.transpose(state_action),probs))\n",
        "\n",
        "    Gt_psi_dot = np.dot(fischer_inv, grad_prob)\n",
        "    fischer_inv -= (alpha * np.outer(Gt_psi_dot,np.transpose(Gt_psi_dot)) ) / (1 - alpha + alpha * np.dot(grad_prob,Gt_psi_dot))\n",
        "    fischer_inv /= (1 - alpha)\n",
        "\n",
        "    weights_w = (1-alpha)*weights_w + alpha*td_error*np.dot(fischer_inv, grad_prob)\n",
        "\n",
        "    weights_p += beta * weights_w\n",
        "\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    actions_list.append(action)\n",
        "\n",
        "    state = new_state\n",
        "    if done==True:\n",
        "      break\n",
        "\n",
        "  returns.append(np.sum(rewards))\n",
        "  #print(np.sum(rewards))\n",
        "  if n_episode%100==0:\n",
        "    print(\"Episode: {:6d}\\tAvg. Return: {:6.2f}\".format(n_episode, np.mean(returns)))\n",
        "  #reinforce_baseline_returns.append(np.mean(returns))\n",
        "  n_episode += 1\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode:    100\tAvg. Return:  19.63\n",
            "Episode:    200\tAvg. Return:  21.22\n",
            "Episode:    300\tAvg. Return:  24.25\n",
            "Episode:    400\tAvg. Return:  24.35\n",
            "Episode:    500\tAvg. Return:  22.77\n",
            "Episode:    600\tAvg. Return:  22.29\n",
            "Episode:    700\tAvg. Return:  24.88\n",
            "Episode:    800\tAvg. Return:  22.90\n",
            "Episode:    900\tAvg. Return:  22.35\n",
            "Episode:   1000\tAvg. Return:  21.06\n",
            "Episode:   1100\tAvg. Return:  20.40\n",
            "Episode:   1200\tAvg. Return:  20.21\n",
            "Episode:   1300\tAvg. Return:  20.73\n",
            "Episode:   1400\tAvg. Return:  19.50\n",
            "Episode:   1500\tAvg. Return:  20.27\n",
            "Episode:   1600\tAvg. Return:  18.74\n",
            "Episode:   1700\tAvg. Return:  18.40\n",
            "Episode:   1800\tAvg. Return:  19.62\n",
            "Episode:   1900\tAvg. Return:  18.78\n",
            "Episode:   2000\tAvg. Return:  20.36\n",
            "Episode:   2100\tAvg. Return:  18.21\n",
            "Episode:   2200\tAvg. Return:  19.30\n",
            "Episode:   2300\tAvg. Return:  20.38\n",
            "Episode:   2400\tAvg. Return:  19.56\n",
            "Episode:   2500\tAvg. Return:  19.54\n",
            "Episode:   2600\tAvg. Return:  21.50\n",
            "Episode:   2700\tAvg. Return:  18.29\n",
            "Episode:   2800\tAvg. Return:  19.22\n",
            "Episode:   2900\tAvg. Return:  17.84\n",
            "Episode:   3000\tAvg. Return:  17.63\n",
            "Episode:   3100\tAvg. Return:  18.02\n",
            "Episode:   3200\tAvg. Return:  17.22\n",
            "Episode:   3300\tAvg. Return:  19.32\n",
            "Episode:   3400\tAvg. Return:  17.89\n",
            "Episode:   3500\tAvg. Return:  19.26\n",
            "Episode:   3600\tAvg. Return:  16.04\n",
            "Episode:   3700\tAvg. Return:  17.05\n",
            "Episode:   3800\tAvg. Return:  17.67\n",
            "Episode:   3900\tAvg. Return:  17.64\n",
            "Episode:   4000\tAvg. Return:  18.94\n",
            "Episode:   4100\tAvg. Return:  17.56\n",
            "Episode:   4200\tAvg. Return:  17.42\n",
            "Episode:   4300\tAvg. Return:  16.77\n",
            "Episode:   4400\tAvg. Return:  19.03\n",
            "Episode:   4500\tAvg. Return:  17.94\n",
            "Episode:   4600\tAvg. Return:  17.76\n",
            "Episode:   4700\tAvg. Return:  16.37\n",
            "Episode:   4800\tAvg. Return:  15.83\n",
            "Episode:   4900\tAvg. Return:  16.90\n",
            "Episode:   5000\tAvg. Return:  17.29\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}