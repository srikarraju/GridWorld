{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSsW0GyI5nt0PYt9SvT7ky",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srikarraju/GridWorld/blob/main/Reinforce_Gridworld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVJapZc6qTRp"
      },
      "source": [
        "import numpy as np\n",
        "class Gridworld:\n",
        "  def __init__(self,grid_dim=4):\n",
        "    self.grid_dim = grid_dim\n",
        "    self.number_states = self.grid_dim*self.grid_dim - 1\n",
        "    self.curr_state = 0\n",
        "\n",
        "  def reset(self):\n",
        "    self.curr_state = 0\n",
        "    return self.curr_state\n",
        "\n",
        "  def step(self,state,action):\n",
        "    curr_state = (int(state/self.grid_dim),state%self.grid_dim)\n",
        "    if action == 0:#Left\n",
        "      if curr_state[1]-1<0:\n",
        "        return state,-1,0\n",
        "      else:\n",
        "        next_state = (curr_state[0],curr_state[1]-1)\n",
        "        if next_state[0]==self.grid_dim-1 and next_state[1]==self.grid_dim-1:\n",
        "          return next_state[0]*4 + next_state[1],0,1\n",
        "        else:\n",
        "          return next_state[0]*4 + next_state[1],-1,0\n",
        "    elif action ==1:#Right\n",
        "      if curr_state[1]+1>=self.grid_dim:\n",
        "        return state,-1,0\n",
        "      else:\n",
        "        next_state = (curr_state[0],curr_state[1]+1)\n",
        "        if next_state[0]==self.grid_dim-1 and next_state[1]==self.grid_dim-1:\n",
        "          return next_state[0]*4 + next_state[1],0,1\n",
        "        else:\n",
        "          return next_state[0]*4 + next_state[1],-1,0\n",
        "    elif action ==2:#Up\n",
        "      if curr_state[0]-1<0:\n",
        "        return state,-1,0\n",
        "      else:\n",
        "        next_state = (curr_state[0]-1,curr_state[1])\n",
        "        if next_state[0]==self.grid_dim-1 and next_state[1]==self.grid_dim-1:\n",
        "          return next_state[0]*4 + next_state[1],0,1\n",
        "        else:\n",
        "          return next_state[0]*4 + next_state[1],-1,0\n",
        "    elif action ==3:#Down\n",
        "      if curr_state[0]+1>=self.grid_dim:\n",
        "        return state,-1,0\n",
        "      else:\n",
        "        next_state = (curr_state[0]+1,curr_state[1])\n",
        "        if next_state[0]==self.grid_dim-1 and next_state[1]==self.grid_dim-1:\n",
        "          return next_state[0]*4 + next_state[1],0,1\n",
        "        else:\n",
        "          return next_state[0]*4 + next_state[1],-1,0\n",
        "    else:\n",
        "      print(\"Invalid Action\")\n",
        "      return None,0,1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj2RjqxDwkoV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import gym\n",
        "from collections import deque"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj2HiXerw8Na"
      },
      "source": [
        "class policy_net(nn.Module):\n",
        "  def __init__(self,state_dim,hidden_dim,action_dim):\n",
        "    super(policy_net,self).__init__()\n",
        "    self.h = nn.Linear(state_dim,hidden_dim)\n",
        "    self.out = nn.Linear(hidden_dim,action_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.h(x))\n",
        "    x = F.softmax(self.out(x),dim=1)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QsLgMmJyCMM"
      },
      "source": [
        "grid_dim = 4\n",
        "num_actions = 4\n",
        "\n",
        "env = Gridworld(grid_dim)\n",
        "\n",
        "policy = policy_net(2,16,4)\n",
        "optimizer = torch.optim.Adam(policy.parameters())\n",
        "\n",
        "returns = deque(maxlen=100)\n",
        "gamma = 0.99"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg62cPW9zH7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee9c704-c318-467f-8a0a-13ce2f946c41"
      },
      "source": [
        "n_episode = 1\n",
        "losses = []\n",
        "reinforce_returns = []\n",
        "#reinforce_baseline_returns = []\n",
        "while n_episode <=10000:\n",
        "  rewards,states,actions = [],[],[]\n",
        "  state = env.reset()\n",
        "  episode_len = 1\n",
        "  while True:\n",
        "    state_vec = np.asarray((int(state/grid_dim),state%grid_dim))\n",
        "    probs = policy(torch.tensor(state_vec).unsqueeze(0).float())\n",
        "    sampler = Categorical(probs)\n",
        "    action = sampler.sample()\n",
        "    new_state, reward, done = env.step(state,action.item())\n",
        "\n",
        "    states.append(state_vec)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "\n",
        "    state = new_state\n",
        "    episode_len += 1\n",
        "    if done==True or episode_len>100:\n",
        "      break\n",
        "\n",
        "  rewards = np.array(rewards)\n",
        "  R = torch.tensor([np.sum(rewards[i:]*(gamma**np.array(range(0,len(rewards)-i)))) for i in range(len(rewards))]).float()\n",
        "  states = torch.tensor(states).float()\n",
        "  actions = torch.tensor(actions)\n",
        "  #state_values = torch.flatten(value_fn(states).float())\n",
        "\n",
        "  probs = policy(states)\n",
        "  sampler = Categorical(probs)\n",
        "  logprobs = -sampler.log_prob(actions)\n",
        "\n",
        "  policy_net_loss = torch.sum(logprobs*R)\n",
        "  #policy_net_loss = torch.sum(logprobs*(R-state_values))\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  policy_net_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  returns.append(np.sum(rewards))\n",
        "  if n_episode%100==0:\n",
        "    print(\"Episode: {:6d}\\tAvg. Return: {:6.2f}\".format(n_episode, np.mean(returns)))\n",
        "  reinforce_returns.append(np.mean(returns))\n",
        "  n_episode += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode:    100\tAvg. Return: -19.90\n",
            "Episode:    200\tAvg. Return: -14.68\n",
            "Episode:    300\tAvg. Return: -11.34\n",
            "Episode:    400\tAvg. Return: -10.14\n",
            "Episode:    500\tAvg. Return:  -9.73\n",
            "Episode:    600\tAvg. Return:  -8.55\n",
            "Episode:    700\tAvg. Return:  -7.90\n",
            "Episode:    800\tAvg. Return:  -7.78\n",
            "Episode:    900\tAvg. Return:  -7.48\n",
            "Episode:   1000\tAvg. Return:  -7.35\n",
            "Episode:   1100\tAvg. Return:  -6.84\n",
            "Episode:   1200\tAvg. Return:  -7.07\n",
            "Episode:   1300\tAvg. Return:  -6.87\n",
            "Episode:   1400\tAvg. Return:  -6.83\n",
            "Episode:   1500\tAvg. Return:  -6.64\n",
            "Episode:   1600\tAvg. Return:  -6.26\n",
            "Episode:   1700\tAvg. Return:  -6.45\n",
            "Episode:   1800\tAvg. Return:  -6.31\n",
            "Episode:   1900\tAvg. Return:  -6.13\n",
            "Episode:   2000\tAvg. Return:  -6.24\n",
            "Episode:   2100\tAvg. Return:  -6.22\n",
            "Episode:   2200\tAvg. Return:  -5.80\n",
            "Episode:   2300\tAvg. Return:  -5.97\n",
            "Episode:   2400\tAvg. Return:  -6.07\n",
            "Episode:   2500\tAvg. Return:  -5.75\n",
            "Episode:   2600\tAvg. Return:  -5.63\n",
            "Episode:   2700\tAvg. Return:  -5.58\n",
            "Episode:   2800\tAvg. Return:  -5.77\n",
            "Episode:   2900\tAvg. Return:  -5.75\n",
            "Episode:   3000\tAvg. Return:  -5.78\n",
            "Episode:   3100\tAvg. Return:  -5.70\n",
            "Episode:   3200\tAvg. Return:  -5.55\n",
            "Episode:   3300\tAvg. Return:  -5.58\n",
            "Episode:   3400\tAvg. Return:  -5.54\n",
            "Episode:   3500\tAvg. Return:  -5.56\n",
            "Episode:   3600\tAvg. Return:  -5.42\n",
            "Episode:   3700\tAvg. Return:  -5.45\n",
            "Episode:   3800\tAvg. Return:  -5.45\n",
            "Episode:   3900\tAvg. Return:  -5.64\n",
            "Episode:   4000\tAvg. Return:  -5.33\n",
            "Episode:   4100\tAvg. Return:  -5.43\n",
            "Episode:   4200\tAvg. Return:  -5.34\n",
            "Episode:   4300\tAvg. Return:  -5.25\n",
            "Episode:   4400\tAvg. Return:  -5.26\n",
            "Episode:   4500\tAvg. Return:  -5.32\n",
            "Episode:   4600\tAvg. Return:  -5.26\n",
            "Episode:   4700\tAvg. Return:  -5.29\n",
            "Episode:   4800\tAvg. Return:  -5.27\n",
            "Episode:   4900\tAvg. Return:  -5.29\n",
            "Episode:   5000\tAvg. Return:  -5.24\n",
            "Episode:   5100\tAvg. Return:  -5.27\n",
            "Episode:   5200\tAvg. Return:  -5.19\n",
            "Episode:   5300\tAvg. Return:  -5.15\n",
            "Episode:   5400\tAvg. Return:  -5.28\n",
            "Episode:   5500\tAvg. Return:  -5.21\n",
            "Episode:   5600\tAvg. Return:  -5.19\n",
            "Episode:   5700\tAvg. Return:  -5.19\n",
            "Episode:   5800\tAvg. Return:  -5.17\n",
            "Episode:   5900\tAvg. Return:  -5.21\n",
            "Episode:   6000\tAvg. Return:  -5.16\n",
            "Episode:   6100\tAvg. Return:  -5.22\n",
            "Episode:   6200\tAvg. Return:  -5.24\n",
            "Episode:   6300\tAvg. Return:  -5.13\n",
            "Episode:   6400\tAvg. Return:  -5.13\n",
            "Episode:   6500\tAvg. Return:  -5.19\n",
            "Episode:   6600\tAvg. Return:  -5.15\n",
            "Episode:   6700\tAvg. Return:  -5.18\n",
            "Episode:   6800\tAvg. Return:  -5.14\n",
            "Episode:   6900\tAvg. Return:  -5.19\n",
            "Episode:   7000\tAvg. Return:  -5.16\n",
            "Episode:   7100\tAvg. Return:  -5.21\n",
            "Episode:   7200\tAvg. Return:  -5.12\n",
            "Episode:   7300\tAvg. Return:  -5.13\n",
            "Episode:   7400\tAvg. Return:  -5.21\n",
            "Episode:   7500\tAvg. Return:  -5.19\n",
            "Episode:   7600\tAvg. Return:  -5.13\n",
            "Episode:   7700\tAvg. Return:  -5.16\n",
            "Episode:   7800\tAvg. Return:  -5.16\n",
            "Episode:   7900\tAvg. Return:  -5.11\n",
            "Episode:   8000\tAvg. Return:  -5.15\n",
            "Episode:   8100\tAvg. Return:  -5.14\n",
            "Episode:   8200\tAvg. Return:  -5.13\n",
            "Episode:   8300\tAvg. Return:  -5.10\n",
            "Episode:   8400\tAvg. Return:  -5.10\n",
            "Episode:   8500\tAvg. Return:  -5.10\n",
            "Episode:   8600\tAvg. Return:  -5.16\n",
            "Episode:   8700\tAvg. Return:  -5.09\n",
            "Episode:   8800\tAvg. Return:  -5.08\n",
            "Episode:   8900\tAvg. Return:  -5.16\n",
            "Episode:   9000\tAvg. Return:  -5.13\n",
            "Episode:   9100\tAvg. Return:  -5.04\n",
            "Episode:   9200\tAvg. Return:  -5.14\n",
            "Episode:   9300\tAvg. Return:  -5.09\n",
            "Episode:   9400\tAvg. Return:  -5.15\n",
            "Episode:   9500\tAvg. Return:  -5.17\n",
            "Episode:   9600\tAvg. Return:  -5.10\n",
            "Episode:   9700\tAvg. Return:  -5.10\n",
            "Episode:   9800\tAvg. Return:  -5.08\n",
            "Episode:   9900\tAvg. Return:  -5.09\n",
            "Episode:  10000\tAvg. Return:  -5.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKypwCh5sZ-8"
      },
      "source": [
        "print(states)\n",
        "print(actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKgpcPsSrA9G",
        "outputId": "66ab0c66-15f8-4280-d949-1f44233e1369"
      },
      "source": [
        "state = env.reset()\n",
        "rewards,states,actions = [],[],[]\n",
        "while True:\n",
        "  print(\"State: \",state)\n",
        "  state_vec = np.asarray((int(state/grid_dim),state%grid_dim))\n",
        "  probs = policy(torch.tensor(state_vec).unsqueeze(0).float())\n",
        "  sampler = Categorical(probs)\n",
        "  action = sampler.sample()\n",
        "  print(\"Action: \",action.item())\n",
        "  new_state, reward, done = env.step(state,action.item())\n",
        "\n",
        "  states.append(state_vec)\n",
        "  actions.append(action)\n",
        "  rewards.append(reward)\n",
        "\n",
        "  state = new_state\n",
        "  if done==True:\n",
        "    break\n",
        "print(states)\n",
        "print(actions)\n",
        "print(rewards)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State:  0\n",
            "Action:  1\n",
            "State:  1\n",
            "Action:  1\n",
            "State:  2\n",
            "Action:  3\n",
            "State:  6\n",
            "Action:  3\n",
            "State:  10\n",
            "Action:  1\n",
            "State:  11\n",
            "Action:  3\n",
            "[array([0, 0]), array([0, 1]), array([0, 2]), array([1, 2]), array([2, 2]), array([2, 3])]\n",
            "[tensor([1]), tensor([1]), tensor([3]), tensor([3]), tensor([1]), tensor([3])]\n",
            "[-1, -1, -1, -1, -1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awCeImqPuPAO",
        "outputId": "eac19c61-210c-4102-f61e-d0645a63d4f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}