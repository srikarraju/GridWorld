{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPLcX9AYNz18MVbvQViyVj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srikarraju/GridWorld/blob/main/REINFORCE_MDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvAbpQIj9FLh"
      },
      "source": [
        "num_states =  4\n",
        "num_actions = 2\n",
        "branching_factor = 2\n",
        "std_dev = 0.1\n",
        "deg_non_stat = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbtgdadC9dGI",
        "outputId": "4d16c77a-8bc7-4e32-e4af-cb57000c35a4"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "\n",
        "rng = default_rng()\n",
        "possible_next_states = []\n",
        "for i in range(num_states):\n",
        "  state_next_states = []\n",
        "  for j in range(num_actions):\n",
        "    next_states = rng.choice(num_states, size=branching_factor, replace=False)\n",
        "    state_next_states.append(next_states[:])\n",
        "  possible_next_states.append(state_next_states)\n",
        "\n",
        "possible_next_states = np.asarray(possible_next_states)\n",
        "print(possible_next_states.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 2, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKmwCW6SBSdR",
        "outputId": "11ef250b-5e1b-4af2-b80f-233da7e3b5c9"
      },
      "source": [
        "trans_probs = []\n",
        "\n",
        "for i in range(num_states):\n",
        "  state_probs = []\n",
        "  for j in range(num_actions):\n",
        "    intervals = np.random.uniform(size = branching_factor - 1)\n",
        "    #print(intervals)\n",
        "    intervals.sort()\n",
        "    state_action_probs = []\n",
        "    state_action_probs.append(intervals[0])\n",
        "    for k in range(branching_factor-2):\n",
        "      state_action_probs.append(intervals[k+1]-intervals[k])\n",
        "    state_action_probs.append(1 - intervals[branching_factor - 2])\n",
        "    state_probs.append(state_action_probs)\n",
        "  trans_probs.append(state_probs)\n",
        "\n",
        "trans_probs = np.asarray(trans_probs)\n",
        "print(trans_probs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 2, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxaofpTfDvJ9",
        "outputId": "30d41a6f-5f10-4bfb-e2bd-01fc7f29bd0b"
      },
      "source": [
        "rewards_mean = []\n",
        "\n",
        "for i in range(num_states):\n",
        "  state_rewards = []\n",
        "  for j in range(num_actions):\n",
        "    state_action_rewards = []\n",
        "    for k in range(branching_factor):\n",
        "      state_action_rewards.append(np.random.normal(0,1))\n",
        "    state_rewards.append(state_action_rewards)\n",
        "  rewards_mean.append(state_rewards)\n",
        "\n",
        "rewards_mean = np.asarray(rewards_mean)\n",
        "print(rewards_mean.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 2, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4NURnUsCJew",
        "outputId": "9f81bf63-8188-4c16-d6a3-d6f196a6f1be"
      },
      "source": [
        "def take_action(state,action):\n",
        "  next_possible_states = possible_next_states[state][action]\n",
        "  next_state_probs = trans_probs[state][action]\n",
        "\n",
        "  #print(next_possible_states,next_state_probs)\n",
        "\n",
        "  next_state_index = np.random.choice(a = np.arange(branching_factor), p = next_state_probs)\n",
        "  next_state = possible_next_states[state][action][next_state_index]\n",
        "  reward_mean = rewards_mean[state][action][next_state_index]\n",
        "\n",
        "  #print(reward_mean)\n",
        "\n",
        "  actual_reward = np.random.normal(reward_mean,std_dev)\n",
        "\n",
        "  return next_state, reward_mean\n",
        "\n",
        "print(take_action(2,1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 0.36335650872061775)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdeDbIFCvphJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69a74f89-191e-485c-9af1-b171e20998e6"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "\n",
        "rng = default_rng()\n",
        "\n",
        "\n",
        "def generate_state_features(num_states,d,l):\n",
        "  state_features = []\n",
        "  product_dict = {}\n",
        "  while len(state_features) < num_states:\n",
        "    one_indices = rng.choice(d, size = l, replace=False)\n",
        "    product = 1\n",
        "    for i in range(l):\n",
        "      product *= one_indices[i]\n",
        "    feature_vec = np.zeros(d)\n",
        "    if product not in product_dict:\n",
        "      for index in one_indices:\n",
        "        feature_vec[index] = 1\n",
        "      state_features.append(feature_vec)\n",
        "      product_dict[product] = 1\n",
        "  #print(len(product_dict))\n",
        "  return state_features\n",
        "\n",
        "print(np.asarray(generate_state_features(num_states,4,2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. 1.]\n",
            " [1. 0. 0. 1.]\n",
            " [0. 1. 0. 1.]\n",
            " [0. 1. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEsHVSbH4nYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f407f2f6-5757-412b-c565-df76d8cb4b69"
      },
      "source": [
        "def generate_state_action_features(state_vec, d, num_actions):\n",
        "  state_action_features = np.zeros(shape=(num_actions,d*num_actions))\n",
        "  for i in range(num_actions):\n",
        "    for j in range(d):\n",
        "      state_action_features[i][d*i+j] = state_vec[j]\n",
        "  return state_action_features\n",
        "\n",
        "print(generate_state_action_features([1,0,0,1,0,0,1,1],8,4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  1. 0. 0. 1. 0. 0. 1. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5DrzYCjcskG"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import gym\n",
        "from collections import deque\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj2HiXerw8Na"
      },
      "source": [
        "class policy_net(nn.Module):\n",
        "  def __init__(self,state_dim,hidden_dim,action_dim):\n",
        "    super(policy_net,self).__init__()\n",
        "    self.h = nn.Linear(state_dim,hidden_dim)\n",
        "    self.out = nn.Linear(hidden_dim,action_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.h(x))\n",
        "    x = F.softmax(self.out(x),dim=1)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QsLgMmJyCMM"
      },
      "source": [
        "d, l = 4, 2\n",
        "policy = policy_net(d,20,num_actions)\n",
        "\n",
        "optimizer = torch.optim.Adam(policy.parameters())\n",
        "\n",
        "gamma = 0.99\n",
        "returns = deque(maxlen=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg62cPW9zH7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dff381c4-e4be-4579-bab3-c2e1e9d7df91"
      },
      "source": [
        "n_episode = 1\n",
        "losses = []\n",
        "reinforce_returns = []\n",
        "\n",
        "state_features = generate_state_features(num_states,d=4,l=2)\n",
        "\n",
        "while n_episode <=5000:\n",
        "  rewards,states,actions = [],[],[]\n",
        "  state = 0\n",
        "  episode_len = 0\n",
        "  while episode_len<=50:\n",
        "    episode_len += 1\n",
        "    probs = policy(torch.tensor(state_features[state]).unsqueeze(0).float())\n",
        "    sampler = Categorical(probs)\n",
        "    action = sampler.sample()\n",
        "    new_state, reward = take_action(state,action)\n",
        "\n",
        "    states.append(state_features[state])\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "\n",
        "    state = new_state\n",
        "\n",
        "\n",
        "  rewards = np.array(rewards)\n",
        "  R = torch.tensor([np.sum(rewards[i:]*(gamma**np.array(range(0,len(rewards)-i)))) for i in range(len(rewards))]).float()\n",
        "  states = torch.tensor(states).float()\n",
        "  actions = torch.tensor(actions)\n",
        "  #state_values = torch.flatten(value_fn(states).float())\n",
        "\n",
        "  probs = policy(states)\n",
        "  sampler = Categorical(probs)\n",
        "  logprobs = -sampler.log_prob(actions)\n",
        "\n",
        "  policy_net_loss = torch.sum(logprobs*R)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  policy_net_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  returns.append(np.sum(rewards))\n",
        "  if n_episode%100==0:\n",
        "    print(\"Episode: {:6d}\\tAvg. Return: {:6.2f}\".format(n_episode, np.mean(returns)))\n",
        "  reinforce_returns.append(np.mean(returns))\n",
        "  n_episode += 1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode:    100\tAvg. Return:   2.84\n",
            "Episode:    200\tAvg. Return:  11.67\n",
            "Episode:    300\tAvg. Return:  17.31\n",
            "Episode:    400\tAvg. Return:  18.91\n",
            "Episode:    500\tAvg. Return:  20.11\n",
            "Episode:    600\tAvg. Return:  21.08\n",
            "Episode:    700\tAvg. Return:  20.71\n",
            "Episode:    800\tAvg. Return:  21.20\n",
            "Episode:    900\tAvg. Return:  22.41\n",
            "Episode:   1000\tAvg. Return:  22.83\n",
            "Episode:   1100\tAvg. Return:  21.38\n",
            "Episode:   1200\tAvg. Return:  20.92\n",
            "Episode:   1300\tAvg. Return:  22.07\n",
            "Episode:   1400\tAvg. Return:  21.71\n",
            "Episode:   1500\tAvg. Return:  23.02\n",
            "Episode:   1600\tAvg. Return:  22.38\n",
            "Episode:   1700\tAvg. Return:  23.24\n",
            "Episode:   1800\tAvg. Return:  22.45\n",
            "Episode:   1900\tAvg. Return:  22.71\n",
            "Episode:   2000\tAvg. Return:  21.78\n",
            "Episode:   2100\tAvg. Return:  22.06\n",
            "Episode:   2200\tAvg. Return:  22.45\n",
            "Episode:   2300\tAvg. Return:  22.38\n",
            "Episode:   2400\tAvg. Return:  23.00\n",
            "Episode:   2500\tAvg. Return:  21.89\n",
            "Episode:   2600\tAvg. Return:  22.39\n",
            "Episode:   2700\tAvg. Return:  23.49\n",
            "Episode:   2800\tAvg. Return:  23.45\n",
            "Episode:   2900\tAvg. Return:  23.16\n",
            "Episode:   3000\tAvg. Return:  23.42\n",
            "Episode:   3100\tAvg. Return:  23.13\n",
            "Episode:   3200\tAvg. Return:  22.47\n",
            "Episode:   3300\tAvg. Return:  22.59\n",
            "Episode:   3400\tAvg. Return:  21.55\n",
            "Episode:   3500\tAvg. Return:  23.89\n",
            "Episode:   3600\tAvg. Return:  22.48\n",
            "Episode:   3700\tAvg. Return:  22.13\n",
            "Episode:   3800\tAvg. Return:  22.51\n",
            "Episode:   3900\tAvg. Return:  22.51\n",
            "Episode:   4000\tAvg. Return:  21.96\n",
            "Episode:   4100\tAvg. Return:  23.77\n",
            "Episode:   4200\tAvg. Return:  23.10\n",
            "Episode:   4300\tAvg. Return:  22.09\n",
            "Episode:   4400\tAvg. Return:  22.16\n",
            "Episode:   4500\tAvg. Return:  23.65\n",
            "Episode:   4600\tAvg. Return:  23.26\n",
            "Episode:   4700\tAvg. Return:  23.37\n",
            "Episode:   4800\tAvg. Return:  23.03\n",
            "Episode:   4900\tAvg. Return:  22.22\n",
            "Episode:   5000\tAvg. Return:  23.39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1A5HeAldTci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2bc18a7-3cb7-48d7-a5fe-b86d6f55f57e"
      },
      "source": [
        "returns = deque(maxlen=100)\n",
        "\n",
        "d, l = 4, 2\n",
        "\n",
        "weights_v = np.zeros(d,dtype=float)\n",
        "weights_p = np.zeros(d*num_actions,dtype=float)\n",
        "\n",
        "\n",
        "print(weights_v)\n",
        "print(weights_p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_CDAvOJ9FbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b19853-97b8-45f6-b723-caa68e278c04"
      },
      "source": [
        "weights_w = np.zeros(d*num_actions,dtype=float)\n",
        "\n",
        "alpha_0, beta_0, gamma, epsilon = 0.1, 0.01, 0.95, 0.1\n",
        "alpha_c, beta_c = 1000, 100000\n",
        "t = 0\n",
        "n_episode = 1\n",
        "actions_list = []\n",
        "avg_reward = 0\n",
        "\n",
        "state_features = generate_state_features(num_states,d=4,l=2)\n",
        "\n",
        "while n_episode <=5000:\n",
        "  rewards,states,actions = [],[],[]\n",
        "  state = 0\n",
        "\n",
        "  episode_len = 0\n",
        "  while episode_len<=50:\n",
        "    episode_len += 1\n",
        "    state_action_features = generate_state_action_features(state_features[state],d,num_actions)\n",
        "    t += 1\n",
        "    probs = np.dot(weights_p,np.transpose(state_action_features))\n",
        "\n",
        "    probs -= probs.max()\n",
        "    probs = np.exp(np.clip(probs/epsilon, -500, 500))\n",
        "    #probs = np.exp(probs)\n",
        "    probs /= probs.sum()\n",
        "\n",
        "    probs2 = probs.cumsum()\n",
        "    action = np.where(probs2 >= np.random.random())[0][0]\n",
        "    #print(action)\n",
        "\n",
        "\n",
        "\n",
        "    new_state, reward = take_action(state,action)\n",
        "\n",
        "\n",
        "    value_curr = np.dot(weights_v,np.asarray(state_features[state]))\n",
        "    value_next = np.dot(weights_v,np.asarray(state_features[new_state]))\n",
        "\n",
        "    avg_reward = (1 - gamma)*avg_reward + gamma * reward\n",
        "\n",
        "    #td_error = reward + value_curr - value_next - avg_reward\n",
        "\n",
        "    td_error = reward + value_curr - value_next\n",
        "\n",
        "    beta = (beta_0 * beta_c) / (beta_c + t)\n",
        "    alpha = (alpha_0 * alpha_c) / (alpha_c + t**(2/3))\n",
        "\n",
        "    weights_v += 0.01*alpha * td_error * np.asarray(state_features[state])\n",
        "\n",
        "    grad_prob = (1 - probs[action]) * np.asarray(state_action_features[action])\n",
        "\n",
        "    weights_w = np.dot((np.eye(d*num_actions) - alpha * np.outer(grad_prob,grad_prob) ),weights_w) + alpha * td_error * grad_prob\n",
        "    weights_p += beta * weights_w\n",
        "\n",
        "\n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    rewards.append(reward)\n",
        "    actions_list.append(action)\n",
        "\n",
        "    state = new_state\n",
        "\n",
        "  returns.append(np.sum(rewards))\n",
        "  #print(np.sum(rewards))\n",
        "  if n_episode%100==0:\n",
        "    print(\"Episode: {:6d}\\tAvg. Return: {:6.2f}\".format(n_episode, np.mean(returns)))\n",
        "  n_episode += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode:    100\tAvg. Return:  23.37\n",
            "Episode:    200\tAvg. Return:  22.80\n",
            "Episode:    300\tAvg. Return:  23.28\n",
            "Episode:    400\tAvg. Return:  23.12\n",
            "Episode:    500\tAvg. Return:  23.38\n",
            "Episode:    600\tAvg. Return:  23.58\n",
            "Episode:    700\tAvg. Return:  22.71\n",
            "Episode:    800\tAvg. Return:  23.56\n",
            "Episode:    900\tAvg. Return:  22.88\n",
            "Episode:   1000\tAvg. Return:  23.46\n",
            "Episode:   1100\tAvg. Return:  22.82\n",
            "Episode:   1200\tAvg. Return:  23.52\n",
            "Episode:   1300\tAvg. Return:  23.41\n",
            "Episode:   1400\tAvg. Return:  23.29\n",
            "Episode:   1500\tAvg. Return:  23.64\n",
            "Episode:   1600\tAvg. Return:  23.82\n",
            "Episode:   1700\tAvg. Return:  23.91\n",
            "Episode:   1800\tAvg. Return:  24.27\n",
            "Episode:   1900\tAvg. Return:  23.45\n",
            "Episode:   2000\tAvg. Return:  24.01\n",
            "Episode:   2100\tAvg. Return:  23.26\n",
            "Episode:   2200\tAvg. Return:  24.02\n",
            "Episode:   2300\tAvg. Return:  23.15\n",
            "Episode:   2400\tAvg. Return:  23.02\n",
            "Episode:   2500\tAvg. Return:  23.82\n",
            "Episode:   2600\tAvg. Return:  23.52\n",
            "Episode:   2700\tAvg. Return:  21.67\n",
            "Episode:   2800\tAvg. Return:  23.96\n",
            "Episode:   2900\tAvg. Return:  23.72\n",
            "Episode:   3000\tAvg. Return:  23.43\n",
            "Episode:   3100\tAvg. Return:  22.50\n",
            "Episode:   3200\tAvg. Return:  23.83\n",
            "Episode:   3300\tAvg. Return:  22.46\n",
            "Episode:   3400\tAvg. Return:  24.30\n",
            "Episode:   3500\tAvg. Return:  21.95\n",
            "Episode:   3600\tAvg. Return:  23.56\n",
            "Episode:   3700\tAvg. Return:  23.56\n",
            "Episode:   3800\tAvg. Return:  22.84\n",
            "Episode:   3900\tAvg. Return:  22.25\n",
            "Episode:   4000\tAvg. Return:  21.94\n",
            "Episode:   4100\tAvg. Return:  23.62\n",
            "Episode:   4200\tAvg. Return:  23.35\n",
            "Episode:   4300\tAvg. Return:  24.49\n",
            "Episode:   4400\tAvg. Return:  22.30\n",
            "Episode:   4500\tAvg. Return:  23.24\n",
            "Episode:   4600\tAvg. Return:  22.73\n",
            "Episode:   4700\tAvg. Return:  23.89\n",
            "Episode:   4800\tAvg. Return:  22.03\n",
            "Episode:   4900\tAvg. Return:  22.91\n",
            "Episode:   5000\tAvg. Return:  22.53\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}